{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import DifferentialEquations.SciMLBase: AbstractDEProblem\n",
    "import SciMLSensitivity: ForwardDiffSensitivity\n",
    "import LinearAlgebra: I, LowerTriangular, logdet\n",
    "import Random\n",
    "import CSV\n",
    "import Lux\n",
    "\n",
    "include(\"lib/population.jl\");\n",
    "include(\"lib/callbacks.jl\");\n",
    "include(\"lib/objectives.jl\");\n",
    "include(\"lib/compartment_models.jl\");\n",
    "\n",
    "using Bijectors\n",
    "using DataFrames\n",
    "using AbstractGPs\n",
    "using ApproximateGPs\n",
    "using DifferentialEquations\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get warfarin data -> Population\n",
    "df = DataFrame(CSV.File(\"../data/warfarin.csv\"))\n",
    "df_group = groupby(df, :ID)\n",
    "\n",
    "indvs = Vector{AbstractIndividual}(undef, length(df_group))\n",
    "for (i, group) in enumerate(df_group)\n",
    "    x = Vector{Float32}(group[1, [:WEIGHT]])\n",
    "    ty = group[(group.DVID .== 1) .& (group.MDV .== 0), [:TIME, :DV]]\n",
    "    ùêà = Matrix{Float32}(group[group.MDV .== 1, [:TIME, :DOSE, :RATE, :DURATION]])\n",
    "    callback = generate_dosing_callback(ùêà)\n",
    "    indvs[i] = Individual(x, Float32.(ty.TIME), Float32.(ty.DV), callback; id = group.ID[1])\n",
    "end\n",
    "population = Population(indvs);\n",
    "println(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "softplus(x::T) where {T<:Real} = log(exp(x) + one(T));\n",
    "softplus_inv(x::T) where {T<:Real} = log(exp(x) - one(T));\n",
    "\n",
    "function build_SVGP(p::NamedTuple; œµ = 1e-6) \n",
    "    kernel = p.hyper[1] * (SqExponentialKernel() ‚àò ScaleTransform(1 / p.hyper[2]))\n",
    "    f = LatentGP(GP(ConstMean(p.hyper[3]), kernel), GaussianLikelihood(), œµ)\n",
    "    q = MvNormal(p.m, p.A)\n",
    "    fz = f(p.z).fx\n",
    "    return SparseVariationalApproximation(fz, q)\n",
    "end\n",
    "\n",
    "function constrain_(p_)\n",
    "    sigma = softplus.(p_.sigma)\n",
    "    Lc = inverse(Bijectors.VecCholeskyBijector(:L))(p_.corr).L\n",
    "    L = sigma .* Lc\n",
    "    A = L * L'\n",
    "    return (hyper = softplus.(p_.hyper), z = p_.z, m = p_.m, A = A, L = L)\n",
    "end\n",
    "\n",
    "function _elbo(obj, prob, population, ps_::AbstractVector{<:NamedTuple}, sigma; N::Int = 100)\n",
    "    ps = constrain_.(ps_)\n",
    "    p = (error = (sigma = softplus.(sigma),),)\n",
    "    gps = build_SVGP.(ps)\n",
    "    f_posts = posterior.(gps)\n",
    "    q_f = reduce(hcat, marginals.([f_post(population.x[1, :] ./ 200.f0) for f_post in f_posts]))\n",
    "    f_Œº = mean.(q_f)'\n",
    "    f_œÉ = std.(q_f)'\n",
    "\n",
    "    LL = zero(eltype(f_Œº))\n",
    "    for i in 1:N\n",
    "        zs = vcat(softplus.(f_Œº + f_œÉ .* randn(eltype(f_Œº), size(f_Œº, 1))), zeros(eltype(f_Œº), 1, size(f_Œº, 2)))\n",
    "        yÃÇ = forward_adjoint_(prob, population, zs)\n",
    "        œÉ¬≤ = variance(obj.error, p, yÃÇ)\n",
    "        LL += ll(Normal, yÃÇ, œÉ¬≤, population.y)\n",
    "    end\n",
    "    return LL / N - sum(prior_kl_.(ps))\n",
    "end\n",
    "\n",
    "prior_kl_(p) = (sum(p.L .^ 2) + p.m'p.m - length(p.m) - logdet(p.A)) / 2\n",
    "\n",
    "function forward_(problem::AbstractDEProblem, individual::AbstractIndividual, z·µ¢::AbstractVecOrMat; get_dv::Bool=false, sensealg=nothing, full::Bool=false, interpolate::Bool=false, saveat_ = is_timevariable(individual) ? individual.t.y : individual.t)\n",
    "    u0 = isempty(individual.initial) ? problem.u0 : individual.initial\n",
    "    saveat = interpolate ? empty(saveat_) : saveat_\n",
    "    save_idxs = full ? (1:length(u0)) : 2\n",
    "    prob = remake(problem, u0 = u0, tspan = (problem.tspan[1], maximum(saveat_)), p = z·µ¢)\n",
    "    interpolate && (individual.callback.save_positions .= 1)\n",
    "    sol = solve(prob, Tsit5(),\n",
    "        save_idxs = save_idxs, saveat = saveat, callback=individual.callback, \n",
    "        tstops=individual.callback.condition.times, sensealg=sensealg\n",
    "    )\n",
    "    interpolate && (individual.callback.save_positions .= 0)\n",
    "    return get_dv ? sol[2, :] : sol\n",
    "end\n",
    "\n",
    "forward_adjoint_(problem::AbstractDEProblem, population::Population, zs::AbstractMatrix) = forward_.((problem,), population, eachcol(zs); full=true, get_dv=true, sensealg=ForwardDiffSensitivity(;convert_tspan=true))\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "type Array has no field beta",
     "output_type": "error",
     "traceback": [
      "type Array has no field beta\n",
      "\n",
      "Stacktrace:\n",
      "  [1] getproperty\n",
      "    @ ./Base.jl:37 [inlined]\n",
      "  [2] constrain_(p_::@NamedTuple{z::Vector{Float64}, m::Vector{Float64}, sigma::Vector{Float64}, corr::Vector{Float64}, hyper::Vector{Float64}})\n",
      "    @ Main ~/Postdoc/Projects/BayesianDCM.jl/src/gpe_ensemble.ipynb:15\n",
      "  [3] _broadcast_getindex_evalf\n",
      "    @ ./broadcast.jl:709 [inlined]\n",
      "  [4] _broadcast_getindex\n",
      "    @ ./broadcast.jl:682 [inlined]\n",
      "  [5] _getindex\n",
      "    @ ./broadcast.jl:706 [inlined]\n",
      "  [6] _broadcast_getindex\n",
      "    @ ./broadcast.jl:681 [inlined]\n",
      "  [7] getindex\n",
      "    @ ./broadcast.jl:636 [inlined]\n",
      "  [8] copy\n",
      "    @ ./broadcast.jl:942 [inlined]\n",
      "  [9] materialize\n",
      "    @ ./broadcast.jl:903 [inlined]\n",
      " [10] _elbo(obj::LogLikelihood{Normal{Float32}, Combined{Tuple{Int64}, typeof(init_sigma)}}, prob::ODEProblem{Vector{Float32}, Tuple{Float32, Float32}, true, Vector{Float32}, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(one_comp_abs!), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, population::Population{Static, BasicIndividual{Int64, Vector{Float32}, Vector{Float32}, Vector{Float32}, DiscreteCallback{var\"#condition#75\"{Vector{Float32}}, var\"#affect!#76\"{Vector{Float32}, Vector{Float32}}, typeof(SciMLBase.INITIALIZE_DEFAULT), typeof(SciMLBase.FINALIZE_DEFAULT)}}}, ps::Vector{@NamedTuple{z::Vector{Float64}, m::Vector{Float64}, sigma::Vector{Float64}, corr::Vector{Float64}, hyper::Vector{Float64}}}, sigma::Vector{Float64}; N::Int64)\n",
      "    @ Main ~/Postdoc/Projects/BayesianDCM.jl/src/gpe_ensemble.ipynb:21\n",
      " [11] _elbo(obj::LogLikelihood{Normal{Float32}, Combined{Tuple{Int64}, typeof(init_sigma)}}, prob::ODEProblem{Vector{Float32}, Tuple{Float32, Float32}, true, Vector{Float32}, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(one_comp_abs!), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, population::Population{Static, BasicIndividual{Int64, Vector{Float32}, Vector{Float32}, Vector{Float32}, DiscreteCallback{var\"#condition#75\"{Vector{Float32}}, var\"#affect!#76\"{Vector{Float32}, Vector{Float32}}, typeof(SciMLBase.INITIALIZE_DEFAULT), typeof(SciMLBase.FINALIZE_DEFAULT)}}}, ps::Vector{@NamedTuple{z::Vector{Float64}, m::Vector{Float64}, sigma::Vector{Float64}, corr::Vector{Float64}, hyper::Vector{Float64}}}, sigma::Vector{Float64})\n",
      "    @ Main ~/Postdoc/Projects/BayesianDCM.jl/src/gpe_ensemble.ipynb:19\n",
      " [12] top-level scope\n",
      "    @ ~/Postdoc/Projects/BayesianDCM.jl/src/gpe_ensemble.ipynb:33"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "prob = ODEProblem(one_comp_abs!, zeros(Float32, 2), (-0.1f0, 144.f0), Float32[])\n",
    "\n",
    "M = 6\n",
    "\n",
    "p1 = (\n",
    "    z = collect(range(0, 1, M)), \n",
    "    m = zeros(M), \n",
    "    sigma = softplus_inv.(fill(0.1, M)), \n",
    "    corr=Bijectors.VecCholeskyBijector(:L)(collect(I(M))), \n",
    "    hyper = softplus_inv.([0.1, 0.1, 0.1])\n",
    ")\n",
    "\n",
    "p2 = (\n",
    "    z = collect(range(0, 1, M)), \n",
    "    m = zeros(M), \n",
    "    sigma = softplus_inv.(fill(0.1, M)), \n",
    "    corr=Bijectors.VecCholeskyBijector(:L)(collect(I(M))), \n",
    "    hyper = softplus_inv.([0.1, 0.1, 0.1])\n",
    ")\n",
    "\n",
    "p3 = (\n",
    "    z = collect(range(0, 1, M)), \n",
    "    m = zeros(M), \n",
    "    sigma = softplus_inv.(fill(0.1, M)), \n",
    "    corr=Bijectors.VecCholeskyBijector(:L)(collect(I(M))), \n",
    "    hyper = softplus_inv.([0.1, 0.1, 0.1])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "obj = LogLikelihood(Combined())\n",
    "_elbo(obj, prob, population, [p1, p2, p3], softplus_inv.([0.1, 0.1]))\n",
    "\n",
    "ps = [p1, p2]\n",
    "p_sigma_init = softplus_inv.([0.1, 0.1])\n",
    "\n",
    "opt = Optimisers.ADAM(0.1)\n",
    "opt_state = Optimisers.setup(opt, ps)\n",
    "opt2 = Optimisers.ADAM(0.1)\n",
    "opt_state2 = Optimisers.setup(opt2, p_sigma_init)\n",
    "\n",
    "for epoch in 1:200\n",
    "    loss, back = Zygote.pullback((ps, p) -> -_elbo(obj, prob, population, ps, p; N=epoch <= 50 ? 10 : 50), ps, p_sigma_init)\n",
    "    grad = back(1)\n",
    "    println(\"Epoch $epoch: loss = $loss\")\n",
    "    opt_state, ps = Optimisers.update(opt_state, ps, first(grad))\n",
    "    opt_state2, p_sigma_init = Optimisers.update(opt_state2, p_sigma_init, last(grad))\n",
    "end\n",
    "\n",
    "gps_opt = build_SVGP.(constrain_.(ps))\n",
    "\n",
    "f_samples_1 = softplus.(rand(posterior(gps_opt[1])(0:0.01:1, 1e-6), 10_000))\n",
    "qs1 = reduce(hcat, [quantile(f_samples_1[i, :], [0.05, 0.95]) for i in 1:size(f_samples_1, 1)])\n",
    "mean1 = mean(f_samples_1, dims=2)[:, 1]\n",
    "Plots.plot(0:0.01:1, mean1, ribbon = (mean1 - qs1[1, :], qs1[2, :] - mean1), label=nothing, color=:black, linewidth=1.4, fillalpha=0.15)\n",
    "\n",
    "f_samples_2 = softplus.(rand(posterior(gps_opt[2])(0:0.01:1, 1e-6), 10_000))\n",
    "qs2 = reduce(hcat, [quantile(f_samples_2[i, :], [0.05, 0.95]) for i in 1:size(f_samples_2, 1)])\n",
    "mean2 = mean(f_samples_2, dims=2)[:, 1]\n",
    "Plots.plot(0:0.01:1, mean2, ribbon = (mean2 - qs2[1, :], qs2[2, :] - mean2), xticks=(0:0.1:1, (0:0.1:1) .* 200), label=nothing, color=:black, linewidth=1.4, fillalpha=0.15)\n",
    ";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
